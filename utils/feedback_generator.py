from langchain_community.chat_models import ChatOllama
from langchain_core.prompts import ChatPromptTemplate

def generate_feedback(summary: str, audio_features: dict, visual_features: dict) -> str:
    prompt_template = ChatPromptTemplate.from_template("""
[ë°œí‘œ ìš”ì•½]
{summary}

[ìŒì„± ë¶„ì„ ê²°ê³¼]
- ì´ ê¸¸ì´: {duration_sec}ì´ˆ
- í‰ê·  í”¼ì¹˜: {avg_pitch}
- ì–µì–‘ ë³€í™”ëŸ‰: {energy_variation}
- ë§ì˜ ì†ë„ (ì¶”ì •): {speech_tempo}

[ì‹œê° í‘œí˜„ ë¶„ì„ ê²°ê³¼]
- ì–¼êµ´ ì¸ì‹ ë¹„ìœ¨: {face_detection_ratio}
- ì œìŠ¤ì²˜ ì‚¬ìš© ë¹„ìœ¨: {gesture_ratio}

ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë°œí‘œì— ëŒ€í•œ ì¢…í•© í”¼ë“œë°±ì„ ì œê³µí•´ì¤˜.
- ë°œí‘œ ë‚´ìš© êµ¬ì„±ê³¼ ì „ë‹¬ë ¥
- ë°œí™” íŠ¹ì„± (ì†ë„, ì–µì–‘ ë“±)
- ë¹„ì–¸ì–´ì  í‘œí˜„ (í‘œì •, ì œìŠ¤ì²˜ ë“±)
- ê°œì„  í¬ì¸íŠ¸ì™€ ì˜í•œ ì ì„ êµ¬ë¶„í•´ì¤˜.
- ë‹¤ë§Œ, ìˆ˜ì¹˜ê°€ 0ì´ë¼ë©´ ì¸ì‹í•˜ì§€ ëª»í–ˆìœ¼ë‹ˆ ë‹¤ë¥¸ ì˜ìƒì„ ì—…ë¡œë“œ ìš”ì²­í•˜ë„ë¡ ìœ ë„í•´ì¤˜.
""")
    llm = ChatOllama(model="qwen2.5vl:7b")
    chain = prompt_template | llm
    return chain.invoke({
        **audio_features,
        **visual_features,
        "summary": summary
    })

if __name__ == "__main__":
    dummy_summary = "ë³¸ ë°œí‘œëŠ” ì„œìš¸ì‹œ ê³µìœ  í‚¥ë³´ë“œ ìš´ì˜ ê°œì„  ë°©ì•ˆì— ëŒ€í•´ ë‹¤ë£¨ì—ˆìŠµë‹ˆë‹¤. ìˆ˜ìš” ì˜ˆì¸¡, ìš´ì˜ íš¨ìœ¨í™”, ë°ì´í„° ê¸°ë°˜ ì •ì±… ë°©í–¥ ë“±ì„ í¬í•¨í–ˆìŠµë‹ˆë‹¤."
    
    dummy_audio_features = {
        "duration_sec": 180,
        "avg_pitch": "220Hz",
        "energy_variation": "ì¤‘ê°„",
        "speech_tempo": "ë¹ ë¦„"
    }

    dummy_visual_features = {
        "face_detection_ratio": "85%",
        "gesture_ratio": "70%"
    }

    feedback = generate_feedback(dummy_summary, dummy_audio_features, dummy_visual_features)
    print("ğŸ¤ ë°œí‘œ í”¼ë“œë°± ê²°ê³¼:\n", feedback.content)
