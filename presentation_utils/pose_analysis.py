import cv2
import mediapipe as mp

# ğŸ¥ ì–¼êµ´, ì œìŠ¤ì²˜ ë“± ë¹„ì–¸ì–´ì  í‘œí˜„ì„ ì˜ìƒì—ì„œ ë¶„ì„
def analyze_visual_features(video_path: str) -> dict:
    cap = cv2.VideoCapture(video_path)

    # MediaPipe ëª¨ë“ˆ ì´ˆê¸°í™”
    mp_face = mp.solutions.face_mesh.FaceMesh(static_image_mode=False)
    mp_pose = mp.solutions.pose.Pose()
    mp_hands = mp.solutions.hands.Hands()

    # í†µê³„ ë³€ìˆ˜ ì´ˆê¸°í™”
    frame_count = 0
    face_detected = 0
    gesture_detected = 0

    # í”„ë ˆì„ ë‹¨ìœ„ë¡œ ì˜ìƒ ë¶„ì„
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # OpenCV â†’ RGB ë³€í™˜
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        # ê° ëª¨ë“ˆì— í”„ë ˆì„ ì „ë‹¬ â†’ ê²°ê³¼ ì¶”ë¡ 
        face_result = mp_face.process(frame_rgb)
        pose_result = mp_pose.process(frame_rgb)
        hands_result = mp_hands.process(frame_rgb)

        # ì–¼êµ´ì´ ì¸ì‹ëœ í”„ë ˆì„ ìˆ˜ ì„¸ê¸°
        if face_result.multi_face_landmarks:
            face_detected += 1

        # ìì„¸ ë˜ëŠ” ì†ë™ì‘ì´ ê°ì§€ëœ í”„ë ˆì„ ìˆ˜ ì„¸ê¸°
        if pose_result.pose_landmarks or hands_result.multi_hand_landmarks:
            gesture_detected += 1

        frame_count += 5 # 5í”„ë ˆì„ë§ˆë‹¤ ì²´í¬(ë¶„ì„ ì†ë„ ìµœì í™”ë¥¼ ìœ„í•¨) => í–¥í›„ pytorchê¸°ë°˜ ì „í™˜ ê³ ë ¤

    cap.release()

    # ê²°ê³¼ í†µê³„ ë°˜í™˜
    return {
        "total_frames": frame_count,
        "face_detected_frames": face_detected,
        "gesture_detected_frames": gesture_detected,
        "face_detection_ratio": round(face_detected / frame_count, 2) if frame_count else 0.0,
        "gesture_ratio": round(gesture_detected / frame_count, 2) if frame_count else 0.0,
    }


# âœ… í…ŒìŠ¤íŠ¸ìš© ì‹¤í–‰ ë¸”ë¡
if __name__ == "__main__":
    import sys
    import os

    # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì˜ìƒ ê²½ë¡œ
    default_video = "uploads/sample_video.mp4"

    # CLI ì¸ì or ê¸°ë³¸ ê²½ë¡œ
    video_path = sys.argv[1] if len(sys.argv) > 1 else default_video

    # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    if not os.path.exists(video_path):
        print(f"ì˜ìƒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {video_path}")
    else:
        print(f"ë¶„ì„ ì¤‘ì¸ ì˜ìƒ íŒŒì¼: {video_path}\n")
        result = analyze_visual_features(video_path)

        # ê²°ê³¼ ì¶œë ¥
        print("ì‹œê° í‘œí˜„ ë¶„ì„ ê²°ê³¼:")
        for k, v in result.items():
            print(f"- {k}: {v}")
