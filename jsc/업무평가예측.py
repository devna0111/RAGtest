"""
performance_model.py
- CSVë¡œ ì—…ë¬´í‰ê°€(ì ìˆ˜ ë˜ëŠ” ë“±ê¸‰) ì˜ˆì¸¡
- ì „ì²˜ë¦¬(ê²°ì¸¡ì¹˜/ìŠ¤ì¼€ì¼/ì›-í•«), í•™ìŠµ/ê²€ì¦, ì¤‘ìš”ë„(í¼ë®¤í…Œì´ì…˜) ì‹œê°í™”, ëª¨ë¸ ì €ì¥/ë¶ˆëŸ¬ì˜¤ê¸°, ì‚¬ìš©ì ì…ë ¥ ì˜ˆì¸¡ê¹Œì§€ í¬í•¨
- ì‚¬ìš©ë²•:
  1) í•™ìŠµ:
     python performance_model.py train --csv_path data/employee.csv --target performance_score --model_path artifacts/model.joblib --imp_png artifacts/feature_importance.png
  2) ì˜ˆì¸¡(ì €ì¥ëœ ëª¨ë¸ë¡œ):
     python performance_model.py predict --model_path artifacts/model.joblib --json '{"age":33,"travel_allowance_per_day":35000,"department":"Sales","overtime_hours":10,"business_trip_days":4,"daily_allowance":20000}'
"""

from __future__ import annotations
import argparse
import json
import os
import warnings
from typing import List, Tuple, Dict, Any

import joblib
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import (
    accuracy_score, f1_score, roc_auc_score, classification_report, confusion_matrix,
    mean_absolute_error, mean_squared_error, r2_score
)
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.inspection import permutation_importance

warnings.filterwarnings("ignore", category=UserWarning)

# =========================
# ğŸ§± ìœ í‹¸ í•¨ìˆ˜ë“¤
# =========================

def detect_problem_type(y: pd.Series) -> str:
    """
    íƒ€ê¹ƒì˜ dtype/ê³ ìœ ê°’ì„ ë³´ê³  ë¬¸ì œ ìœ í˜• ìë™ íŒë³„
    - ìˆ«ìí˜• + ê³ ìœ ê°’ì´ ë§ìœ¼ë©´ íšŒê·€
    - ê·¸ ì™¸ëŠ” ë¶„ë¥˜
    """
    if pd.api.types.is_numeric_dtype(y):
        # ìˆ«ìí˜•ì¸ë° ìœ ë‹ˆí¬ ê°’ì´ ì ìœ¼ë©´ ë“±ê¸‰í˜• ì ìˆ˜ë¡œ ë¶„ë¥˜ì¼ ìˆ˜ë„ ìˆìŒ
        n_unique = y.nunique(dropna=True)
        if n_unique <= 10:
            return "classification"
        return "regression"
    else:
        return "classification"


def split_features(df: pd.DataFrame, target: str) -> Tuple[List[str], List[str]]:
    """
    ë°ì´í„°í”„ë ˆì„ì—ì„œ ìˆ«ì/ë²”ì£¼í˜• ì»¬ëŸ¼ ìë™ ë¶„ë¦¬ (íƒ€ê¹ƒ ì œì™¸)
    """
    feature_cols = [c for c in df.columns if c != target]
    num_cols = [c for c in feature_cols if pd.api.types.is_numeric_dtype(df[c])]
    cat_cols = [c for c in feature_cols if not pd.api.types.is_numeric_dtype(df[c])]
    return num_cols, cat_cols


def build_preprocess(num_cols: List[str], cat_cols: List[str]) -> ColumnTransformer:
    """
    ìˆ«ì/ë²”ì£¼í˜• ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì„±
    - ìˆ«ì: ê²°ì¸¡ì¹˜ í‰ê·  ëŒ€ì¹˜ + í‘œì¤€í™”
    - ë²”ì£¼: ê²°ì¸¡ì¹˜ 'missing' ëŒ€ì¹˜ + ì›-í•« ì¸ì½”ë”©
    """
    num_pipe = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="mean")),
        ("scaler", StandardScaler())
    ])
    cat_pipe = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("ohe", OneHotEncoder(handle_unknown="ignore", sparse=False))
    ])
    pre = ColumnTransformer(
        transformers=[
            ("num", num_pipe, num_cols),
            ("cat", cat_pipe, cat_cols)
        ],
        remainder="drop"
    )
    return pre


def pick_model(problem_type: str):
    """
    ë¬¸ì œ ìœ í˜•ë³„ ê¸°ë³¸ ëª¨ë¸ ì„ íƒ
    - íšŒê·€: RandomForestRegressor
    - ë¶„ë¥˜: RandomForestClassifier
    """
    if problem_type == "regression":
        return RandomForestRegressor(
            n_estimators=300,
            max_depth=None,
            random_state=42,
            n_jobs=-1
        )
    else:
        return RandomForestClassifier(
            n_estimators=300,
            max_depth=None,
            class_weight="balanced",
            random_state=42,
            n_jobs=-1
        )


def evaluate_classification(y_true, y_pred, y_proba=None, label="TEST"):
    """
    ë¶„ë¥˜ ì„±ëŠ¥ ì§€í‘œ ì¶œë ¥
    """
    print(f"\n===== ğŸ“Š Classification Metrics ({label}) =====")
    print("Accuracy:", round(accuracy_score(y_true, y_pred), 4))
    print("F1 (weighted):", round(f1_score(y_true, y_pred, average="weighted"), 4))
    try:
        # ì´ì§„ ë¶„ë¥˜ì¼ ë•Œë§Œ ROC-AUC ì‹œë„
        if y_proba is not None and y_proba.shape[1] == 2:
            auc = roc_auc_score(y_true, y_proba[:, 1])
            print("ROC-AUC:", round(auc, 4))
    except Exception:
        pass
    print("\nClassification report:\n", classification_report(y_true, y_pred))
    print("Confusion matrix:\n", confusion_matrix(y_true, y_pred))


def evaluate_regression(y_true, y_pred, label="TEST"):
    """
    íšŒê·€ ì„±ëŠ¥ ì§€í‘œ ì¶œë ¥
    """
    mae = mean_absolute_error(y_true, y_pred)
    rmse = mean_squared_error(y_true, y_pred, squared=False)
    r2 = r2_score(y_true, y_pred)
    print(f"\n===== ğŸ“ˆ Regression Metrics ({label}) =====")
    print("MAE :", round(mae, 4))
    print("RMSE:", round(rmse, 4))
    print("RÂ²  :", round(r2, 4))


def plot_permutation_importance(
    pipeline: Pipeline,
    X_test: pd.DataFrame,
    y_test: pd.Series,
    problem_type: str,
    top_k: int = 20,
    save_png: str | None = None,
    feature_names: List[str] | None = None
):
    """
    í¼ë®¤í…Œì´ì…˜ ì¤‘ìš”ë„(ëª¨ë¸ ë¶ˆê°€ì§€ë¡ ì ) ê³„ì‚° ë° ë§‰ëŒ€ ê·¸ë˜í”„ ì €ì¥
    - ì „ì²˜ë¦¬ í¬í•¨ íŒŒì´í”„ë¼ì¸ì— ëŒ€í•´ ì•ˆì „í•˜ê²Œ ì‚¬ìš© ê°€ëŠ¥
    """
    print("\nğŸ” Calculating permutation importance (this can take a while)...")
    scoring = "neg_mean_squared_error" if problem_type == "regression" else "f1_weighted"
    result = permutation_importance(
        pipeline, X_test, y_test,
        n_repeats=10,
        random_state=42,
        n_jobs=-1,
        scoring=scoring
    )
    importances = result.importances_mean
    stds = result.importances_std

    # ì¤‘ìš”ë„ ìƒìœ„ ì •ë ¬
    idx = np.argsort(importances)[::-1]
    idx = idx[:top_k]

    # íŠ¹ì„± ì´ë¦„ ì¶”ì : ColumnTransformer ì´í›„ íŠ¹ì„±ëª… ë³µì›
    if feature_names is None:
        try:
            ohe = pipeline.named_steps["model"].__class__.__name__  # dummy to use try
        except Exception:
            pass

    # ì‹¤ì œë¡œëŠ” ì „ì²˜ë¦¬ ë³€í™˜ í›„ ì¹¼ëŸ¼ëª…ì´ ë°”ë€Œì§€ë§Œ, ì—¬ê¸°ì„  ì›ë³¸ í”¼ì²˜ ë‹¨ìœ„ ì¤‘ìš”ë„ë¥¼ ë³´ëŠ” ìš©ë„ë¡œ
    # permutation_importanceëŠ” ì…ë ¥ X_test ê¸°ì¤€ìœ¼ë¡œ í”¼ì²˜ë¥¼ ì„ìœ¼ë¯€ë¡œ ì›ë³¸ ì¹¼ëŸ¼ëª… ì‚¬ìš© ê°€ëŠ¥
    plotted_names = list(X_test.columns)

    plt.figure(figsize=(9, max(4, int(len(idx) * 0.4))))
    plt.barh(range(len(idx)), importances[idx], xerr=stds[idx])
    plt.yticks(range(len(idx)), [plotted_names[i] for i in idx])
    plt.gca().invert_yaxis()
    plt.title("Permutation Importance (higher = more important)")
    plt.xlabel("Importance")
    plt.tight_layout()
    if save_png:
        os.makedirs(os.path.dirname(save_png), exist_ok=True)
        plt.savefig(save_png, dpi=160)
        print(f"ğŸ’¾ ì¤‘ìš”ë„ ê·¸ë˜í”„ ì €ì¥: {save_png}")
    else:
        plt.show()
    plt.close()


def cross_validate(pipeline: Pipeline, X: pd.DataFrame, y: pd.Series, problem_type: str):
    """
    ë¹ ë¥¸ 5-Fold êµì°¨ê²€ì¦ ì ìˆ˜ í™•ì¸
    """
    if problem_type == "regression":
        scoring = "r2"
    else:
        scoring = "f1_weighted"
    scores = cross_val_score(pipeline, X, y, cv=5, scoring=scoring, n_jobs=-1)
    print(f"\nğŸ” 5-Fold CV ({scoring}) -> mean={scores.mean():.4f} Â± {scores.std():.4f}")


# =========================
# ğŸ§  í•™ìŠµ í•¨ìˆ˜ (end-to-end)
# =========================

def train(
    csv_path: str,
    target: str = "performance_score",
    test_size: float = 0.2,
    random_state: int = 42,
    model_path: str = "artifacts/model.joblib",
    importance_png: str | None = "artifacts/feature_importance.png"
):
    """
    CSVë¡œë¶€í„° í•™ìŠµ â†’ ê²€ì¦ â†’ ì¤‘ìš”ë„ ì €ì¥ â†’ íŒŒì´í”„ë¼ì¸ ì €ì¥
    """
    print(f"ğŸ“¥ Load CSV: {csv_path}")
    df = pd.read_csv(csv_path)

    if target not in df.columns:
        raise ValueError(f"íƒ€ê¹ƒ ì»¬ëŸ¼ '{target}' ì´(ê°€) CSVì— ì—†ìŠµë‹ˆë‹¤. ì»¬ëŸ¼ ëª©ë¡: {list(df.columns)}")

    # íƒ€ê¹ƒ/í”¼ì²˜ ë¶„ë¦¬
    y = df[target]
    X = df.drop(columns=[target])

    # ë¬¸ì œ ìœ í˜• ê²°ì •
    problem_type = detect_problem_type(y)
    print(f"ğŸ” Detected problem type: {problem_type}")

    # ìˆ«ì/ë²”ì£¼í˜• ë¶„í• 
    num_cols, cat_cols = split_features(df, target)

    # ì „ì²˜ë¦¬ + ëª¨ë¸ íŒŒì´í”„ë¼ì¸
    pre = build_preprocess(num_cols, cat_cols)
    model = pick_model(problem_type)

    pipe = Pipeline(steps=[
        ("preprocess", pre),
        ("model", model)
    ])

    # í•™ìŠµ/ê²€ì¦ ë¶„ë¦¬ (ë¶„ë¥˜ë©´ stratify)
    stratify = y if problem_type == "classification" else None
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state, stratify=stratify
    )

    # í•™ìŠµ
    print("\nğŸš€ Training...")
    pipe.fit(X_train, y_train)

    # í‰ê°€
    print("\nâœ… Evaluation on TEST:")
    y_pred = pipe.predict(X_test)

    if problem_type == "classification":
        try:
            y_proba = pipe.predict_proba(X_test)
        except Exception:
            y_proba = None
        evaluate_classification(y_test, y_pred, y_proba, label="TEST")
    else:
        evaluate_regression(y_test, y_pred, label="TEST")

    # êµì°¨ê²€ì¦
    cross_validate(pipe, X, y, problem_type)

    # ì¤‘ìš”ë„ ì €ì¥ (ì›ë³¸ X_test ì¹¼ëŸ¼ëª… ì‚¬ìš©)
    plot_permutation_importance(
        pipeline=pipe,
        X_test=X_test,
        y_test=y_test,
        problem_type=problem_type,
        save_png=importance_png,
        feature_names=list(X.columns)
    )

    # íŒŒì´í”„ë¼ì¸ + ë©”íƒ€ì •ë³´ ì €ì¥
    meta = {
        "problem_type": problem_type,
        "target": target,
        "num_cols": num_cols,
        "cat_cols": cat_cols,
        "feature_columns": list(X.columns)
    }
    os.makedirs(os.path.dirname(model_path), exist_ok=True)
    joblib.dump({"pipeline": pipe, "meta": meta}, model_path)
    print(f"\nğŸ’¾ Model saved to: {model_path}")

    return pipe, meta


# =========================
# ğŸ”® ì˜ˆì¸¡ í•¨ìˆ˜
# =========================

def predict_with_saved_model(model_path: str, payload: Dict[str, Any]) -> Dict[str, Any]:
    """
    ì €ì¥ëœ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ë‹¨ê±´ ì˜ˆì¸¡
    - payload: ì‚¬ìš©ì ì…ë ¥ ë”•ì…”ë„ˆë¦¬ (CSVì˜ í”¼ì²˜ ì»¬ëŸ¼ëª…ê³¼ ë™ì¼í•´ì•¼ í•¨)
    """
    bundle = joblib.load(model_path)
    pipe: Pipeline = bundle["pipeline"]
    meta = bundle["meta"]

    # ì…ë ¥ ìœ íš¨ì„± ì²´í¬ ë° DataFrame ë³€í™˜
    X_cols = meta["feature_columns"]
    x_df = pd.DataFrame([payload], columns=X_cols)  # ëˆ„ë½ ì»¬ëŸ¼ì€ NaNìœ¼ë¡œ ë“¤ì–´ê°

    # ì˜ˆì¸¡
    y_pred = pipe.predict(x_df)

    result = {
        "problem_type": meta["problem_type"],
        "prediction": y_pred[0]
    }

    # ë¶„ë¥˜ì¼ ê²½ìš° í™•ë¥ ë„ ì œê³µ
    if meta["problem_type"] == "classification":
        try:
            proba = pipe.predict_proba(x_df)[0]
            classes = pipe.named_steps["model"].classes_
            result["proba"] = {str(c): float(p) for c, p in zip(classes, proba)}
        except Exception:
            pass

    return result


# =========================
# ğŸ–¥ï¸ CLI ì§„ì…ì 
# =========================

def main():
    parser = argparse.ArgumentParser(description="Work Performance Prediction (Regression/Classification auto)")
    sub = parser.add_subparsers(dest="cmd", required=True)

    # train
    p_train = sub.add_parser("train", help="Train a model from CSV")
    p_train.add_argument("--csv_path", required=True, type=str)
    p_train.add_argument("--target", default="performance_score", type=str)
    p_train.add_argument("--test_size", default=0.2, type=float)
    p_train.add_argument("--random_state", default=42, type=int)
    p_train.add_argument("--model_path", default="artifacts/model.joblib", type=str)
    p_train.add_argument("--imp_png", default="artifacts/feature_importance.png", type=str)

    # predict
    p_pred = sub.add_parser("predict", help="Predict with saved model")
    p_pred.add_argument("--model_path", required=True, type=str)
    p_pred.add_argument("--json", required=True, type=str, help="JSON string of feature values")

    args = parser.parse_args()

    if args.cmd == "train":
        train(
            csv_path=args.csv_path,
            target=args.target,
            test_size=args.test_size,
            random_state=args.random_state,
            model_path=args.model_path,
            importance_png=args.imp_png
        )

    elif args.cmd == "predict":
        payload = json.loads(args.json)
        res = predict_with_saved_model(args.model_path, payload)
        print(json.dumps(res, ensure_ascii=False, indent=2))


if __name__ == "__main__":
    main()
